import os
import streamlit as st
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferMemory
from langchain_openai import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains.question_answering import load_qa_chain
from langchain_chroma import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.prompts import PromptTemplate
from datetime import datetime

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
OPENAI_API_KEY = "[API Key ì…ë ¥]"
DATA_PATH = "./data"
CHROMA_PATH = "./chroma_db"

def load_documents():
    documents = []
    for root, _, files in os.walk(DATA_PATH):
        for file in files:
            if file.endswith('.pdf'):
                pdf_path = os.path.join(root, file)
                print(f"ğŸ“„ PDF ë¡œë“œ ì¤‘: {pdf_path}")

                loader = PyPDFLoader(pdf_path)
                docs = loader.load()

                print(f"ğŸ”¹ {file}ì—ì„œ {len(docs)}ê°œì˜ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
                documents.extend(docs)
    
    return documents

def create_chroma_db():
    documents = load_documents()
    print(f"ğŸ“š ì´ {len(documents)}ê°œì˜ ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    docs = text_splitter.split_documents(documents)
    print(f"ğŸ” ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: ì´ {len(docs)}ê°œì˜ ì²­í¬ ìƒì„±ë¨")
    
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY) # text-embedding-ada-002
    db = Chroma.from_documents(docs, embeddings, persist_directory=CHROMA_PATH)
    print("âœ… ChromaDB ì €ì¥ ì™„ë£Œ!")
    return db

def get_qa_chain_with_memory(db):
    """ê³¼ê±° ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ëŠ” QA ì²´ì¸ ìƒì„±"""
    retriever = db.as_retriever(search_kwargs={"k": 10})
    llm = ChatOpenAI(model_name="gpt-4o", openai_api_key=OPENAI_API_KEY, temperature=0.5)
    
    # ë§ì¶¤í˜• í”„ë¡¬í”„íŠ¸ ì„¤ì •
    custom_prompt = PromptTemplate(
        input_variables=["context", "question"], # ì…ë ¥ ë³€ìˆ˜ ì •ì˜
        template=(
            """
                ë‹¹ì‹ ì€ ì€í–‰ ëŒ€ì¶œ ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤.  
                ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê³  ìì„¸í•˜ê²Œ ëŒ€ì¶œ ìƒí’ˆì„ ì•ˆë‚´í•˜ë©°,  
                ì „ì„¸ìê¸ˆëŒ€ì¶œê³¼ ì£¼íƒë‹´ë³´ëŒ€ì¶œì— ëŒ€í•œ ì •ë³´ë¥¼ ì •í™•í•˜ê³  ì‹ ë¢°ì„± ìˆê²Œ ì œê³µí•©ë‹ˆë‹¤. 
                ë‹¤ìŒ 'ì§ˆë¬¸'ì— ëŒ€í•´ ì œê³µëœ 'ë¬¸ì„œ'ì˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ìƒì„±í•´ ì£¼ì„¸ìš”. 
                ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

                ì§ˆë¬¸:  
                {question}  

                ë¬¸ì„œ:  
                {context}  

                ë‹µë³€:  
            """
        )
    )

    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm, 
        retriever=retriever, 
        chain_type="stuff",
        memory=memory,
        chain_type_kwargs={
            "prompt": custom_prompt,
            "verbose": False
        }
    )

    def custom_run(query):
        retrieved_docs = retriever.get_relevant_documents(query)
        print(f"\nğŸ” ì§ˆë¬¸: {query}")
        print(f"ğŸ“‚ ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: {len(retrieved_docs)}")

        bank_products = {}
        sources = set()

        for doc in retrieved_docs: 
            bank_name = doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ").split(os.sep)[-2]
            sources.add(doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ"))

            if bank_name not in bank_products:
                bank_products[bank_name] = []

            bank_products[bank_name].append(doc.page_content[:700]) # ìµœëŒ€  700ì ê¹Œì§€ ì €ì¥

        bank_info_text = "**ì€í–‰ë³„ ëŒ€ì¶œ ìƒí’ˆ ì •ë³´:**\n"
        for bank, products in bank_products.items():
            bank_info_text += f"\nğŸ¦ **{bank.upper()}**\n"
            for i, product in enumerate(products, start=1):
                bank_info_text += f"{i}. {product}...\n"

        modified_query = f"{query}\n\n{bank_info_text}"
        print(modified_query)
        response = qa_chain.run(modified_query)

        response_text = f"ğŸ’¡ **AI ë‹µë³€:**\n{response}\n\nğŸ“Œ **ì¶œì²˜:** {', '.join(sources)}"
        return response_text

    return custom_run

# feedback function
def generate_feedback(history, user_feedback, qa_chain):
    conversation_log = "\n".join([f"ì‚¬ìš©ì: {q}\nAI: {a}" for q, a in history])
    evaluation_prompt = f"""
        ë‹¹ì‹ ì€ AI ëŒ€í™” í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ëŒ€í™”ë¥¼ í‰ê°€í•˜ì„¸ìš”.

        ### ëŒ€í™” ë‚´ì—­ ###
        {conversation_log}

        ### ì‚¬ìš©ì í”¼ë“œë°± ###
        '{user_feedback}' (ğŸ‘ ë§Œì¡± / ğŸ¤” ë³´í†µ / ğŸ‘ ê°œì„  í•„ìš”)

        ### í‰ê°€ ê¸°ì¤€ ###
        - ì •ë³´ì˜ ëª…í™•ì„±(1~5)
        - ë‹µë³€ì˜ ëª…í™•ì„±(1~5)
        - ì‚¬ìš©ì ì¹œí™”ì„±(1~5)
        - ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„

        ì‚¬ìš©ì í”¼ë“œë°±ì„ ê³ ë ¤í•˜ì—¬ AIì˜ ê°•ì ê³¼ ê°œì„ ì ì„ ë¶„ì„í•˜ê³ , ë³´ì™„í•  ì ì„ ì œì•ˆí•˜ì„¸ìš”.
    """

    # llm = ChatOpenAI(model_name="gpt-4o", openai_api_key=OPENAI_API_KEY, temperature=0)
    # return llm.predict(evaluation_prompt)

    return qa_chain(evaluation_prompt) #ê¸°ì¡´ ì²´ì¸ì— í•™ìŠµ

def main():
    st.markdown(
        "<h2 style='text-align: center;'>ğŸ’¬ ì€í–‰ ì „ì„¸ìê¸ˆëŒ€ì¶œ ë° ì£¼íƒë‹´ë³´ëŒ€ì¶œ Q&A ì±—ë´‡</h2>",
        unsafe_allow_html=True
    )
    st.write("ì€í–‰ ì „ì„¸ìê¸ˆëŒ€ì¶œ ë° ì£¼íƒë‹´ë³´ëŒ€ì¶œ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!")

    if not os.path.exists(CHROMA_PATH):
        st.info("ë¬¸ì„œ DBë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”...")
        db = create_chroma_db()
    else:
        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
        db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)

    qa_chain = get_qa_chain_with_memory(db)

    user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
    
    if st.button("ì§ˆë¬¸í•˜ê¸°") and (user_input not in ["ëŒ€í™” ì¢…ë£Œ"]):
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            response = qa_chain(user_input)  # ê³¼ê±° ëŒ€í™” ë°˜ì˜ë¨
            st.success("ğŸ“ ë‹µë³€:")
            st.write(response)  # PDF ì¶œì²˜ í¬í•¨

            # ì‚¬ìš©ì ì§ˆë¬¸ ë° ë‹µë³€ ì €ì¥
            if "history" not in st.session_state:
                st.session_state.history = []
            st.session_state.history.append((user_input, response))

    if user_input in ["ëŒ€í™” ì¢…ë£Œ"]:
        st.success("ğŸ˜Š ë„ì›€ì´ ë˜ì—ˆë‹¤ë‹ˆ ê¸°ì©ë‹ˆë‹¤! ì¶”ê°€ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”.")
        st.write("ğŸ‘‰ í˜¹ì‹œ ì´ë²ˆ ë‹µë³€ì´ ë§Œì¡±ìŠ¤ëŸ¬ì› ë‚˜ìš”? ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ í”¼ë“œë°±ì„ ë‚¨ê²¨ì£¼ì„¸ìš”!")

        # í”¼ë“œë°± ë²„íŠ¼ ì¶”ê°€
        user_feedback = st.radio("ë‹µë³€ì´ ë§Œì¡±ìŠ¤ëŸ¬ì› ë‚˜ìš”?", ("ğŸ‘ ë§Œì¡±", "ğŸ¤” ë³´í†µ", "ğŸ‘ ê°œì„  í•„ìš”"), index=None)
        if user_feedback:
            if "feedback_processed" not in st.session_state or not st.session_state.feedback_processed:
                with st.spinner("ëŒ€í™” í‰ê°€ ì¤‘..."):
                    feedback = generate_feedback(st.session_state.history, user_feedback, qa_chain)
                    st.write("ğŸ§ AI ìì²´ í‰ê°€ ê²°ê³¼:")
                    st.write(f"âœ… í”¼ë“œë°±ì´ ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤: {feedback}")


                    st.session_state.feedback_processed = True
                    
            if "history" in st.session_state and st.session_state.history:
                conversation_log = "\n".join([f"ì‚¬ìš©ì: {q}\nAI: {a}" for q, a in st.session_state.history])
                
                today = datetime.today().strftime('%Y-%m-%d')
                download_filename = f"{today}_conversation_history.txt"
                st.download_button(
                    label="ëŒ€í™” ë‚´ìš© ë‹¤ìš´ë¡œë“œí•˜ê¸°",
                    data=conversation_log,
                    file_name=download_filename,
                    mime="text/plain"
                )

    if "history" in st.session_state and st.session_state.history:
        st.write("### ğŸ“œ ì´ì „ ëŒ€í™” ê¸°ë¡")
        for question, answer in st.session_state.history:
            st.write(f"**Q:** {question}")
            st.write(f"**A:** {answer}")
            st.write("---")

if __name__ == "__main__":
    main()
